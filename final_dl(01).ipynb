{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4762fa2c-dca4-49d5-8814-92d2c43b93af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ad22b231-8cb4-481d-8ac4-79ebbdb5e72b",
   "metadata": {},
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f17173d0-d830-4075-a461-1cfe1b6aab3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6b0851-7e47-45f1-8e4f-07f70101b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55a12589-c500-4d6f-ac6f-ad7756c44081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfb52445-86c0-4d54-99bf-198f95a714b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the new directory for saving results\n",
    "new_output_dir = 'C:/Users/91894/OneDrive/Desktop/fake review/Results'\n",
    "new_logging_dir = 'C:/Users/91894/OneDrive/Desktop/fake review/logs'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1adae3b-9335-4a5f-af25-7e8638bc3857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./Results\",  # Use a relative path instead of C:/Users...\n",
    "    logging_dir=\"./logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Reduce batch size\n",
    "    per_device_eval_batch_size=4,\n",
    "    logging_steps=1000,  # Reduce logging frequency\n",
    "    save_steps=2000,  # Save checkpoints less often\n",
    "    save_total_limit=1,  # Keep only 1 checkpoint\n",
    "    fp16=True if torch.cuda.is_available() else False,  # Use mixed precision if GPU is available\n",
    "    report_to=\"none\"  # Disable unnecessary logging (e.g., WandB)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c10aa8a5-ba07-4b22-93d5-9cfb8f21eca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_dir = 'C:/Users/91894/OneDrive/Desktop/fake review/Results'\n",
    "new_logging_dir = 'C:/Users/91894/OneDrive/Desktop/fake review/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0070453-6a0a-47c4-b74c-2c46926a95c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set a new working directory for the notebook\n",
    "os.chdir('C:/Users/91894/OneDrive/Desktop/fake review')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dba84e-640a-4842-a37d-63d334136365",
   "metadata": {},
   "source": [
    "### Classification using deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f93f067d-0822-4abe-aa67-b1665f509059",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, SimpleRNN\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454ba8c7-3835-4ed7-938b-1a1d9d135f4b",
   "metadata": {},
   "source": [
    " #### load data and then encode the labels into numeric values using LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6dbdfdeb-fa50-4f6c-b824-bb58fffd32c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"C:/Users/91894/OneDrive/Desktop/fake review/TP_DS.csv\", nrows=10000)\n",
    "\n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f8ee55-218d-40af-a34a-6a078b95501d",
   "metadata": {},
   "source": [
    "#### split the data into training and testing sets, tokenize the text, pad the sequences, and build and train three types of neural network models: RNN, LSTM, and BiLSTM for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "15755c67-7708-41af-99dc-20561f1217e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91894\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 30ms/step - accuracy: 0.5758 - loss: 0.6692 - precision: 0.5888 - recall: 0.4638 - val_accuracy: 0.7256 - val_loss: 0.5602 - val_precision: 0.6722 - val_recall: 0.8966\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 24ms/step - accuracy: 0.7993 - loss: 0.4690 - precision: 0.7719 - recall: 0.8394 - val_accuracy: 0.8175 - val_loss: 0.4274 - val_precision: 0.7928 - val_recall: 0.8670\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.8888 - loss: 0.2998 - precision: 0.8695 - recall: 0.9130 - val_accuracy: 0.8150 - val_loss: 0.4378 - val_precision: 0.8404 - val_recall: 0.7845\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 25ms/step - accuracy: 0.8894 - loss: 0.3015 - precision: 0.8933 - recall: 0.8791 - val_accuracy: 0.7775 - val_loss: 0.5161 - val_precision: 0.9254 - val_recall: 0.6108\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.9206 - loss: 0.2192 - precision: 0.9247 - recall: 0.9163 - val_accuracy: 0.8119 - val_loss: 0.5144 - val_precision: 0.8763 - val_recall: 0.7328\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Classification Report for RNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.88      0.78      0.83      1008\n",
      "          OR       0.80      0.89      0.84       992\n",
      "\n",
      "    accuracy                           0.84      2000\n",
      "   macro avg       0.84      0.84      0.84      2000\n",
      "weighted avg       0.84      0.84      0.84      2000\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91894\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 43ms/step - accuracy: 0.5116 - loss: 0.6932 - precision_1: 0.5220 - recall_1: 0.4293 - val_accuracy: 0.5369 - val_loss: 0.6703 - val_precision_1: 0.8447 - val_recall_1: 0.1071\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 43ms/step - accuracy: 0.5351 - loss: 0.7126 - precision_1: 0.5840 - recall_1: 0.3231 - val_accuracy: 0.5225 - val_loss: 0.6771 - val_precision_1: 0.9444 - val_recall_1: 0.0628\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 45ms/step - accuracy: 0.5390 - loss: 0.6704 - precision_1: 0.7997 - recall_1: 0.1203 - val_accuracy: 0.5238 - val_loss: 0.6759 - val_precision_1: 0.9310 - val_recall_1: 0.0665\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 41ms/step - accuracy: 0.5164 - loss: 0.6695 - precision_1: 0.5427 - recall_1: 0.5213 - val_accuracy: 0.5144 - val_loss: 0.6979 - val_precision_1: 0.8723 - val_recall_1: 0.0505\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "Classification Report for LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.53      0.98      0.69      1008\n",
      "          OR       0.88      0.12      0.21       992\n",
      "\n",
      "    accuracy                           0.56      2000\n",
      "   macro avg       0.70      0.55      0.45      2000\n",
      "weighted avg       0.70      0.56      0.45      2000\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91894\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 61ms/step - accuracy: 0.6341 - loss: 0.6112 - precision_2: 0.6603 - recall_2: 0.5509 - val_accuracy: 0.8581 - val_loss: 0.3198 - val_precision_2: 0.8707 - val_recall_2: 0.8461\n",
      "Epoch 2/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - accuracy: 0.9049 - loss: 0.2220 - precision_2: 0.9082 - recall_2: 0.9003 - val_accuracy: 0.8769 - val_loss: 0.3021 - val_precision_2: 0.8764 - val_recall_2: 0.8818\n",
      "Epoch 3/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 67ms/step - accuracy: 0.9439 - loss: 0.1482 - precision_2: 0.9463 - recall_2: 0.9434 - val_accuracy: 0.8825 - val_loss: 0.3027 - val_precision_2: 0.9021 - val_recall_2: 0.8621\n",
      "Epoch 4/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 64ms/step - accuracy: 0.9672 - loss: 0.0986 - precision_2: 0.9666 - recall_2: 0.9677 - val_accuracy: 0.8819 - val_loss: 0.3225 - val_precision_2: 0.9030 - val_recall_2: 0.8596\n",
      "Epoch 5/10\n",
      "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 61ms/step - accuracy: 0.9800 - loss: 0.0633 - precision_2: 0.9837 - recall_2: 0.9765 - val_accuracy: 0.8675 - val_loss: 0.3851 - val_precision_2: 0.8886 - val_recall_2: 0.8448\n",
      "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step\n",
      "Classification Report for BiLSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.89      0.86      0.88      1008\n",
      "          OR       0.86      0.89      0.88       992\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splits the data into training and testing sets\n",
    "X = data['cleaned_text']\n",
    "y = data['label_encoded']\n",
    "X = data['cleaned_text'].astype(str)  # Convert to string to handle any float or NaN issues\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Text Preprocessing with Tokenization and Padding\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "#model building\n",
    "# Define a function to build models\n",
    "def build_model(model_type=\"RNN\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(64, return_sequences=False))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(64, return_sequences=False))\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "        \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "# Training  of model\n",
    "def train_and_evaluate(model_type):\n",
    "    model = build_model(model_type)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train_pad, y_train, \n",
    "                        epochs=10, \n",
    "                        batch_size=64, \n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluation of model\n",
    "    y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "    print(f\"Classification Report for {model_type}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['CG', 'OR']))\n",
    "    \n",
    "# loop through Train and evaluate RNN, LSTM, and BiLSTM models\n",
    "for model_type in [\"RNN\", \"LSTM\", \"BiLSTM\"]:\n",
    "    train_and_evaluate(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8cf09e-d332-48e1-9042-b80fac2421a5",
   "metadata": {},
   "source": [
    "## Model Training, Prediction, and Evaluation Using BERT for Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468239f4-f4ae-4c38-a5bf-94e2cb51c5be",
   "metadata": {},
   "source": [
    "### fine-tune a BERT model for sequence classification using the Transformers library by Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0095e68-f684-44b1-bd43-086d82a19da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\91894\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (0.27.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91894\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a759085b-0330-4e74-ba56-708f13c6d0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\91894\\anaconda3\\lib\\site-packages (4.50.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.5)\n",
      "Requirement already satisfied: torch>=2.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.2.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\91894\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91894\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\91894\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22af06ab-5987-4f97-99a5-c9f22a66c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abb8e1a-4c97-410e-a6af-36c6c2ba7641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4ab19a9-63ef-4743-a1b3-624c2101194a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\91894\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\91894\\AppData\\Local\\Temp\\ipykernel_21032\\1403537024.py:80: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='600' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [600/600 31:23, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.667660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.893672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.506800</td>\n",
       "      <td>1.338719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.75      0.80      0.77       104\n",
      "          OR       0.76      0.71      0.74        96\n",
      "\n",
      "    accuracy                           0.76       200\n",
      "   macro avg       0.76      0.75      0.75       200\n",
      "weighted avg       0.76      0.76      0.75       200\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "device = torch.device(\"cpu\")  # Force CPU usage\n",
    "\n",
    "# Set device to CPU only\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Print the device being used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset and preprocess\n",
    "data = pd.read_csv(\"C:/Users/91894/OneDrive/Desktop/fake review/TP_DS.csv\", nrows=1000)   # Replace with your file path\n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])\n",
    "\n",
    "# Split dataset\n",
    "X = data['cleaned_text'].astype(str)\n",
    "y = data['label_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class to handle text and labels\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0).to(device),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0).to(device),\n",
    "            'labels': torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }\n",
    "\n",
    "# Create dataset and data loaders\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ReviewDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(fp16=True, \n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=4,  # Lower batch size for CPU\n",
    "    per_device_eval_batch_size=4,  # Lower batch size for CPU\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    save_total_limit=1,  # Save only the last two checkpoints\n",
    "    save_strategy='epoch',  # Save model at the end of each epoch\n",
    "    disable_tqdm=False,  # Enable progress bar in console\n",
    ")\n",
    "\n",
    "# Define Trainer for training BERT model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1).numpy()\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, pred_labels, target_names=['CG', 'OR']))\n",
    "\n",
    "import gc\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94c3c289-3df0-4d1b-be51-221e1f6cce1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and training arguments saved in BERT./saved_model\n"
     ]
    }
   ],
   "source": [
    "## saving  trained model, tokenizer, and training arguments\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define a directory to save the model and tokenizer\n",
    "model_dir = 'BERT./saved_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained BERT model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# Save training arguments as a JSON file\n",
    "with open(os.path.join(model_dir, 'training_args.json'), 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "print(f\"Model, tokenizer, and training arguments saved in {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fca9fe13-c221-48bf-8e23-ff931ab8e206",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "#### The model is highly effective at detecting fake reviews with strong performance across both classes. With an accuracy of 76%, it strikes a good balance between precision and recall, showing that it is both accurate and reliable in classifying reviews as genuine or fake. This result is promising for the task of fake review detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7cc904-c21e-48d1-a0a6-1468bb7eb06b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
