{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f836e39d-7edb-4fa8-b47c-306fec47395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DURGA\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Specify the new directory for saving results\n",
    "new_output_dir = 'E:/Final Project/Results'\n",
    "new_logging_dir = 'E:/Final Project/logs'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=new_output_dir,  # Directory to save the model checkpoints\n",
    "    logging_dir=new_logging_dir,  # Directory to save logs\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_steps=500,  # Interval for logging\n",
    "    save_steps=500,  # Interval for saving checkpoints\n",
    "    save_total_limit=2  # Limit the number of checkpoints to keep\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7824ef3-75f0-4169-a1cb-2542268d2dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_output_dir = 'E:/Final Project/Results'\n",
    "new_logging_dir = 'E:/Final Project/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59ebc6cb-d49a-4e1c-ab22-3377e5bf42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Set a new working directory for the notebook\n",
    "os.chdir('E:/Final Project')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48535d58-26e9-4542-bb2b-83cf12a741f5",
   "metadata": {},
   "source": [
    "### Classification using deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db6c71a7-3625-4e00-aedc-3663525ec47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional, SimpleRNN\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, SpatialDropout1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aed587e-0bf4-47cc-9e4b-9be9ee7a3ccb",
   "metadata": {},
   "source": [
    " #### load data and then encode the labels into numeric values using LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0493bb66-c7f7-4f9d-9d7b-cadcdcd53ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\DURGA\\Desktop\\final_project_fake_review\\dataset\\TP_DS.csv\")  \n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857b73c1-dd5c-4f0d-b5f6-c6fe8fae8867",
   "metadata": {},
   "source": [
    "#### split the data into training and testing sets, tokenize the text, pad the sequences, and build and train three types of neural network models: RNN, LSTM, and BiLSTM for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "783cb232-8f6c-4f12-b21e-9ffd5f83475c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DURGA\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 52ms/step - accuracy: 0.5658 - loss: 0.6750 - precision: 0.5720 - recall: 0.5483 - val_accuracy: 0.7196 - val_loss: 0.5761 - val_precision: 0.7077 - val_recall: 0.7518\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 45ms/step - accuracy: 0.6518 - loss: 0.6034 - precision: 0.6451 - recall: 0.6594 - val_accuracy: 0.4962 - val_loss: 0.6903 - val_precision: 0.4965 - val_recall: 0.2593\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 49ms/step - accuracy: 0.5585 - loss: 0.6719 - precision: 0.5650 - recall: 0.5167 - val_accuracy: 0.6987 - val_loss: 0.6022 - val_precision: 0.6844 - val_recall: 0.7419\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 57ms/step - accuracy: 0.7228 - loss: 0.5632 - precision: 0.7160 - recall: 0.7431 - val_accuracy: 0.7347 - val_loss: 0.5588 - val_precision: 0.7279 - val_recall: 0.7530\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 64ms/step - accuracy: 0.7608 - loss: 0.5061 - precision: 0.7356 - recall: 0.8156 - val_accuracy: 0.7272 - val_loss: 0.5723 - val_precision: 0.7090 - val_recall: 0.7743\n",
      "Epoch 6/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 68ms/step - accuracy: 0.7943 - loss: 0.4693 - precision: 0.7876 - recall: 0.7992 - val_accuracy: 0.7469 - val_loss: 0.5470 - val_precision: 0.7403 - val_recall: 0.7638\n",
      "Epoch 7/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 52ms/step - accuracy: 0.8158 - loss: 0.4333 - precision: 0.8094 - recall: 0.8273 - val_accuracy: 0.7400 - val_loss: 0.5646 - val_precision: 0.7217 - val_recall: 0.7844\n",
      "Epoch 8/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 53ms/step - accuracy: 0.7426 - loss: 0.4981 - precision: 0.7634 - recall: 0.6936 - val_accuracy: 0.7034 - val_loss: 0.5802 - val_precision: 0.7648 - val_recall: 0.5907\n",
      "Epoch 9/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 48ms/step - accuracy: 0.7960 - loss: 0.4544 - precision: 0.7938 - recall: 0.7952 - val_accuracy: 0.8454 - val_loss: 0.3709 - val_precision: 0.8756 - val_recall: 0.8066\n",
      "Epoch 10/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 47ms/step - accuracy: 0.8996 - loss: 0.2658 - precision: 0.8998 - recall: 0.8994 - val_accuracy: 0.8680 - val_loss: 0.3305 - val_precision: 0.8771 - val_recall: 0.8571\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 9ms/step\n",
      "Classification Report for RNN:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.86      0.87      0.86      4016\n",
      "          OR       0.87      0.86      0.86      4071\n",
      "\n",
      "    accuracy                           0.86      8087\n",
      "   macro avg       0.86      0.86      0.86      8087\n",
      "weighted avg       0.86      0.86      0.86      8087\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DURGA\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 82ms/step - accuracy: 0.5290 - loss: 0.6800 - precision_1: 0.5489 - recall_1: 0.3670 - val_accuracy: 0.5559 - val_loss: 0.6556 - val_precision_1: 0.9652 - val_recall_1: 0.1195\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 79ms/step - accuracy: 0.7050 - loss: 0.5720 - precision_1: 0.7162 - recall_1: 0.6910 - val_accuracy: 0.7776 - val_loss: 0.5122 - val_precision_1: 0.8343 - val_recall_1: 0.6948\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 80ms/step - accuracy: 0.7019 - loss: 0.5520 - precision_1: 0.7813 - recall_1: 0.5513 - val_accuracy: 0.8147 - val_loss: 0.4336 - val_precision_1: 0.8495 - val_recall_1: 0.7666\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 81ms/step - accuracy: 0.8248 - loss: 0.4039 - precision_1: 0.8640 - recall_1: 0.7673 - val_accuracy: 0.8264 - val_loss: 0.4181 - val_precision_1: 0.8428 - val_recall_1: 0.8041\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 83ms/step - accuracy: 0.8718 - loss: 0.3275 - precision_1: 0.9050 - recall_1: 0.8270 - val_accuracy: 0.8766 - val_loss: 0.3100 - val_precision_1: 0.8891 - val_recall_1: 0.8617\n",
      "Epoch 6/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 80ms/step - accuracy: 0.9067 - loss: 0.2292 - precision_1: 0.9331 - recall_1: 0.8751 - val_accuracy: 0.8751 - val_loss: 0.2856 - val_precision_1: 0.9002 - val_recall_1: 0.8448\n",
      "Epoch 7/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 80ms/step - accuracy: 0.9133 - loss: 0.2033 - precision_1: 0.9398 - recall_1: 0.8829 - val_accuracy: 0.8950 - val_loss: 0.2687 - val_precision_1: 0.8910 - val_recall_1: 0.9011\n",
      "Epoch 8/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 80ms/step - accuracy: 0.9312 - loss: 0.1699 - precision_1: 0.9347 - recall_1: 0.9275 - val_accuracy: 0.8983 - val_loss: 0.2644 - val_precision_1: 0.8831 - val_recall_1: 0.9190\n",
      "Epoch 9/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 86ms/step - accuracy: 0.9503 - loss: 0.1301 - precision_1: 0.9537 - recall_1: 0.9458 - val_accuracy: 0.8967 - val_loss: 0.2781 - val_precision_1: 0.8974 - val_recall_1: 0.8968\n",
      "Epoch 10/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 82ms/step - accuracy: 0.9608 - loss: 0.1038 - precision_1: 0.9647 - recall_1: 0.9572 - val_accuracy: 0.8953 - val_loss: 0.3043 - val_precision_1: 0.8954 - val_recall_1: 0.8962\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 21ms/step\n",
      "Classification Report for LSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.91      0.86      0.89      4016\n",
      "          OR       0.87      0.92      0.90      4071\n",
      "\n",
      "    accuracy                           0.89      8087\n",
      "   macro avg       0.89      0.89      0.89      8087\n",
      "weighted avg       0.89      0.89      0.89      8087\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DURGA\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 145ms/step - accuracy: 0.7559 - loss: 0.4607 - precision_2: 0.7782 - recall_2: 0.6975 - val_accuracy: 0.9020 - val_loss: 0.2277 - val_precision_2: 0.8936 - val_recall_2: 0.9135\n",
      "Epoch 2/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 136ms/step - accuracy: 0.9221 - loss: 0.1870 - precision_2: 0.9218 - recall_2: 0.9227 - val_accuracy: 0.9059 - val_loss: 0.2123 - val_precision_2: 0.8990 - val_recall_2: 0.9153\n",
      "Epoch 3/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 136ms/step - accuracy: 0.9491 - loss: 0.1310 - precision_2: 0.9480 - recall_2: 0.9510 - val_accuracy: 0.9153 - val_loss: 0.2208 - val_precision_2: 0.9086 - val_recall_2: 0.9242\n",
      "Epoch 4/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 129ms/step - accuracy: 0.9634 - loss: 0.0955 - precision_2: 0.9663 - recall_2: 0.9606 - val_accuracy: 0.9127 - val_loss: 0.2498 - val_precision_2: 0.8944 - val_recall_2: 0.9366\n",
      "Epoch 5/10\n",
      "\u001b[1m405/405\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 131ms/step - accuracy: 0.9743 - loss: 0.0685 - precision_2: 0.9735 - recall_2: 0.9752 - val_accuracy: 0.9066 - val_loss: 0.2698 - val_precision_2: 0.9162 - val_recall_2: 0.8959\n",
      "\u001b[1m253/253\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 23ms/step\n",
      "Classification Report for BiLSTM:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.91      0.89      0.90      4016\n",
      "          OR       0.90      0.91      0.91      4071\n",
      "\n",
      "    accuracy                           0.90      8087\n",
      "   macro avg       0.90      0.90      0.90      8087\n",
      "weighted avg       0.90      0.90      0.90      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# splits the data into training and testing sets\n",
    "X = data['cleaned_text']\n",
    "y = data['label_encoded']\n",
    "X = data['cleaned_text'].astype(str)  # Convert to string to handle any float or NaN issues\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#  Text Preprocessing with Tokenization and Padding\n",
    "vocab_size = 10000\n",
    "max_length = 100\n",
    "embedding_dim = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "#model building\n",
    "# Define a function to build models\n",
    "def build_model(model_type=\"RNN\"):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "    if model_type == \"RNN\":\n",
    "        model.add(SimpleRNN(64, return_sequences=False))\n",
    "    elif model_type == \"LSTM\":\n",
    "        model.add(LSTM(64, return_sequences=False))\n",
    "    elif model_type == \"BiLSTM\":\n",
    "        model.add(Bidirectional(LSTM(64, return_sequences=False)))\n",
    "        \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy', Precision(), Recall()])\n",
    "    return model\n",
    "\n",
    "# Training  of model\n",
    "def train_and_evaluate(model_type):\n",
    "    model = build_model(model_type)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "    \n",
    "    history = model.fit(X_train_pad, y_train, \n",
    "                        epochs=10, \n",
    "                        batch_size=64, \n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[early_stopping])\n",
    "    \n",
    "    # Evaluation of model\n",
    "    y_pred = (model.predict(X_test_pad) > 0.5).astype(\"int32\")\n",
    "    print(f\"Classification Report for {model_type}:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['CG', 'OR']))\n",
    "    \n",
    "# loop through Train and evaluate RNN, LSTM, and BiLSTM models\n",
    "for model_type in [\"RNN\", \"LSTM\", \"BiLSTM\"]:\n",
    "    train_and_evaluate(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658180d-e252-48c6-ab49-fbd2718abf54",
   "metadata": {},
   "source": [
    "## Model Training, Prediction, and Evaluation Using BERT for Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e8ee83-1e90-4203-9ec6-7da47c9d59b3",
   "metadata": {},
   "source": [
    "### fine-tune a BERT model for sequence classification using the Transformers library by Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1f03056c-ee4d-4178-8428-8ed60ecbb8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
      "Requirement already satisfied: torch in c:\\users\\durga\\anaconda3\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: torchvision in c:\\users\\durga\\anaconda3\\lib\\site-packages (0.20.1+cu124)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\durga\\anaconda3\\lib\\site-packages (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torchvision) (10.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "41ed1109-48e0-4f0d-b472-d99c5e13a489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\durga\\anaconda3\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (23.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: filelock in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: requests in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\durga\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface-hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8935ba59-5bc6-43c9-8f2e-dcffc37d2ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in c:\\users\\durga\\anaconda3\\lib\\site-packages (4.46.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.32.2)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.20.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (2.5.1+cu124)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from transformers[torch]) (1.1.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\durga\\anaconda3\\lib\\site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\durga\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers[torch]) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\durga\\anaconda3\\lib\\site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "83298763-0ce6-4f94-8217-e5733275e5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DURGA\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\DURGA\\AppData\\Local\\Temp\\ipykernel_2640\\403905814.py:73: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='12132' max='12132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [12132/12132 38:42:00, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.369200</td>\n",
       "      <td>0.412140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.260900</td>\n",
       "      <td>0.290170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.177100</td>\n",
       "      <td>0.291100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          CG       0.92      0.93      0.93      4016\n",
      "          OR       0.93      0.92      0.93      4071\n",
      "\n",
      "    accuracy                           0.93      8087\n",
      "   macro avg       0.93      0.93      0.93      8087\n",
      "weighted avg       0.93      0.93      0.93      8087\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Check if CUDA (GPU) is available,The model will run on GPU if available, otherwise on CPU.\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Print the device being used (CPU or GPU)\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load dataset and preprocess\n",
    "data = pd.read_csv(r\"C:\\Users\\DURGA\\Desktop\\final_project_fake_review\\dataset\\TP_DS.csv\")  # Replace with your file path\n",
    "data['label_encoded'] = LabelEncoder().fit_transform(data['label'])\n",
    "\n",
    "# Split dataset\n",
    "X = data['cleaned_text'].astype(str)\n",
    "y = data['label_encoded']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize BERT tokenizer\n",
    "#Converts the text into token IDs that can be understood by the BERT model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom Dataset class to handle text and labels\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):#Limits the length of the input text to 128 tokens\n",
    "        self.texts = texts                                       #You can adjust this depending on the dataset and model requirements.\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten().to(device),\n",
    "            'attention_mask': encoding['attention_mask'].flatten().to(device),\n",
    "            'labels': torch.tensor(label, dtype=torch.long).to(device)\n",
    "        }\n",
    "\n",
    "# Create dataset and data loaders\n",
    "train_dataset = ReviewDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = ReviewDataset(X_test, y_test, tokenizer)\n",
    "\n",
    "# Load pre-trained BERT model for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy='epoch',\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "# Define Trainer for training BERT model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "pred_labels = torch.argmax(torch.tensor(predictions.predictions), axis=1).numpy()\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, pred_labels, target_names=['CG', 'OR']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0e98553d-0c4f-4140-9cfb-4812da16e448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, tokenizer, and training arguments saved in BERT./saved_model\n"
     ]
    }
   ],
   "source": [
    "## saving  trained model, tokenizer, and training arguments\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Define a directory to save the model and tokenizer\n",
    "model_dir = 'BERT./saved_model'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save the trained BERT model\n",
    "model.save_pretrained(model_dir)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(model_dir)\n",
    "\n",
    "# Save training arguments as a JSON file\n",
    "with open(os.path.join(model_dir, 'training_args.json'), 'w') as f:\n",
    "    json.dump(training_args.to_dict(), f)\n",
    "\n",
    "print(f\"Model, tokenizer, and training arguments saved in {model_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da77ff8-5b52-4fd9-991c-14ede226f7ac",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "#### The model is highly effective at detecting fake reviews with strong performance across both classes. With an accuracy of 93%, it strikes a good balance between precision and recall, showing that it is both accurate and reliable in classifying reviews as genuine or fake. This result is promising for the task of fake review detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50a2719-d7d4-4007-a45f-1c742c2a908d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
